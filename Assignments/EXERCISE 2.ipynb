{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703aa216",
   "metadata": {},
   "source": [
    "# Numerical Mathematics - Assignment 2\n",
    "\n",
    "## Peder Brekke, Simen Nesland and Espen Bj√∏rge Urheim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed886f",
   "metadata": {},
   "source": [
    "### PROBLEM 1\n",
    "\n",
    "#### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd809e",
   "metadata": {},
   "source": [
    "1) $$f: \\mathbb{R} \\rightarrow \\mathbb{R} \\text{, with } f(\\boldsymbol{x}) = \\|\\boldsymbol{x}\\|$$\n",
    "\n",
    "We know that\n",
    "$$\\nabla f(\\boldsymbol{x}) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \n",
    "\\ldots , \\frac{\\partial f}{\\partial x_n}\\right)$$\n",
    "\n",
    "We choose to use the 2-norm for our computations. Looking at $f$ we find\n",
    "\n",
    "$$f(\\boldsymbol{x}) = \\|\\boldsymbol{x}\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\ldots + x_n^2} \n",
    "= \\sqrt{\\sum_{i=1}^{n} x_i^2}$$\n",
    "\n",
    "If we study for example $x_1$ we get\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1}  = \\frac{\\partial}{\\partial x_1} \\left(\\sum_{i=1}^{n} x_i^2\\right)^{1/2}\n",
    "= \\frac{\\frac{\\partial }{\\partial x_1} \\left(x_1^2 + x_2^2 + \\ldots + x_n^2 \\right)}{2\\|\\boldsymbol{x}\\|}\n",
    "= \\frac{x_1}{\\|\\boldsymbol{x}\\|}$$\n",
    "\n",
    "This is obviously a general result, hence\n",
    "\n",
    "$$\\nabla f(\\boldsymbol{x}) = \\frac{\\boldsymbol{x}}{\\|\\boldsymbol{x}\\|} $$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb958244",
   "metadata": {},
   "source": [
    "2) $$F(\\boldsymbol{x}) = A\\boldsymbol{x}$$\n",
    "\n",
    "$$A\\boldsymbol{x} =  \\begin{bmatrix}a_{11}&a_{12}&\\cdots&a_{1n}\\\\a_{21}&\\ddots&&\\vdots\\\\\\vdots&&&\\\\a_{n1}&\\cdots&&a_{nn}\\end{bmatrix}\n",
    "\\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{bmatrix}\n",
    "=\\begin{bmatrix}a_{11}x_1+a_{12}x_2+\\ldots+a_{1n}x_n\\\\\n",
    "a_{21}x_1+a_{22}x_2+\\ldots+a_{2n}x_n\\\\\\vdots\\\\\n",
    "a_{n1}x_1+a_{n2}x_2+\\ldots+a_{nn}x_n\\end{bmatrix}$$\n",
    "\n",
    "The Jacobi is given by\n",
    "$$\\boldsymbol{J_F} (\\boldsymbol{x}) = \\begin{bmatrix}\\frac{\\partial F_1}{\\partial x_1} &\\cdots&\\frac{\\partial F_n}{\\partial x_1}\\\\\\vdots&\\ddots&\\vdots\\\\\\frac{\\partial F_1}{\\partial x_n}&\\cdots&\\frac{\\partial F_n}{\\partial x_n}\\end{bmatrix}$$\n",
    "\n",
    "If we look at the general case, we find that\n",
    "\n",
    "$$\\frac{\\partial F_i}{\\partial x_r} = \\frac{\\partial}{\\partial x_r} \\left( \\sum_{j=1}^{n} a_{ij} x_j \\right) \n",
    "= \\sum_{j=1}^{n} a_{ij} \\frac{\\partial x_j}{\\partial x_r}\n",
    "= \\sum_{j=1}^{n} a_{ij} \\delta_{rj} = a_{ir}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\\boldsymbol{J_F} (\\boldsymbol{x}) = A$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bfc803",
   "metadata": {},
   "source": [
    "3) $$F(\\boldsymbol{x}) = \\|A\\boldsymbol{x}\\|^2 = \\boldsymbol{x}^T A^T A \\boldsymbol{x}$$\n",
    "\n",
    "Introducing $M = A^TA$ and using Einstein notation we get\n",
    "\n",
    "$$\\nabla F(\\boldsymbol{x})_r \n",
    "= \\frac{\\partial}{\\partial x_r}(x_jM_{ji}x_i) \n",
    "= \\frac{\\partial x_j}{\\partial x_r}M_{ji}x_i + x_j M_{ji} \\frac{\\partial x_i}{\\partial x_r}\n",
    "= \\delta_{jr} M_{ji}x_i + x_j M_{ji} \\delta_{ir}\n",
    "= M_{ri}x_i + x_j M_{jr}$$\n",
    "\n",
    "Now, since these are just numbers/components we can freely change the orders. Also, we know that from the definition of $M$, $M=M^T$. This yields\n",
    "\n",
    "$$\\nabla F(\\boldsymbol{x})_r\n",
    "= M_{ri}x_i + M_{rj}x_j\n",
    "= (A^TA)_{ri}x_i + (A^TA)_{rj}x_j\n",
    "= 2(A^TA)_{ri}x_i = 2(A^TAx)_r$$\n",
    "\n",
    "The penultimate equality comes from $j$ and $i$ beeing arbitrary indices of summation. Finally, since $r$ also was an arbitrary component in the gradient, we get\n",
    "\n",
    "$$\\nabla F(\\boldsymbol{x}) = 2A^TA\\boldsymbol{x}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f82cd0",
   "metadata": {},
   "source": [
    "4) $$F(\\boldsymbol{x}) = \\boldsymbol{x}^T A^T \\boldsymbol{x}$$\n",
    "\n",
    "$$\\nabla f(\\boldsymbol{x})_r = \\frac{\\partial}{\\partial x_r} (x_j A_{ji} x_i)\n",
    "= \\delta_{jr} A_{ji}x_i + x_j A_{ji} \\delta_{ir}\n",
    "= A_{ri}x_i + x_jA_{jr} = (Ax)_r + (A^Tx)_r$$\n",
    "\n",
    "As for 3), this yields\n",
    "\n",
    "$$\\nabla f(\\boldsymbol{x}) = (A+A^T)\\boldsymbol{x}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c45c3",
   "metadata": {},
   "source": [
    "#### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a4dc6",
   "metadata": {},
   "source": [
    "If $A$ is symmetric i.e. $A = A^T$, it is quite obvious the gradient in $4)$ simplifies. Just inserting the property gives\n",
    "\n",
    "$$\\nabla f(\\boldsymbol{x}) = (A+A^T)\\boldsymbol{x} = (A+A)\\boldsymbol{x} = 2A\\boldsymbol{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0e530c",
   "metadata": {},
   "source": [
    "#### c)\n",
    "\n",
    "Computing the hessian for $3)$ and $4)$ simplifies a lot when $A$ is symmetric. Also the result from $2)$ comes in handy.\n",
    "***\n",
    "For $3)$ we find\n",
    "\n",
    "$$\\nabla^2 f(\\boldsymbol{x}) = \\nabla(2A^2\\boldsymbol{x}) = 2\\nabla(A^2 \\boldsymbol{x})$$\n",
    "\n",
    "We remember from $2)$ that for a function given by $f(\\boldsymbol{x}) = A\\boldsymbol{x}$ where $A$ is a $n \\times n$ matrix, the jacobian is just $A$ itself. In this case our $A$ is just $A^2$ hence\n",
    "\n",
    "$$\\nabla^2 f(\\boldsymbol{x}) = 2A^2$$\n",
    "\n",
    "***\n",
    "In the case of $4)$ we get\n",
    "\n",
    "$$\\nabla^2 f(\\boldsymbol{x}) = \\nabla(2A\\boldsymbol{x})$$\n",
    "\n",
    "Using the same argument as befor we find\n",
    "\n",
    "$$\\nabla^2 f(\\boldsymbol{x}) = 2A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea1d33d",
   "metadata": {},
   "source": [
    "#### d)\n",
    "\n",
    "$$f(\\boldsymbol{x}) = \\frac{1}{2} \\| A\\boldsymbol{x} - \\boldsymbol{y} \\|_2^2 + \\mu \\| \\boldsymbol{x} \\|_2^2 $$\n",
    "\n",
    "We attempt to simplifiy the expression so that we can utilize the previous results\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f(\\boldsymbol{x}) \n",
    "&= \\frac{1}{2} \\| A\\boldsymbol{x} - \\boldsymbol{y} \\|_2^2 + \\mu \\| \\boldsymbol{x} \\|_2^2 \\\\\n",
    "&= \\frac{1}{2} (A\\boldsymbol{x} - \\boldsymbol{y})^T (A\\boldsymbol{x}-\\boldsymbol{y}) + \\mu\\boldsymbol{x}^T\\boldsymbol{x} \\\\\n",
    "&= \\frac{1}{2} (\\boldsymbol{x}^T A^T - \\boldsymbol{y}^T) (A\\boldsymbol{x}-\\boldsymbol{y}) + \\mu\\boldsymbol{x}^T\\boldsymbol{x} \\\\\n",
    "&= \\frac{1}{2} (\\boldsymbol{x}^T A^T A \\boldsymbol{x} - \\boldsymbol{x}^T A^T \\boldsymbol{y} - \\boldsymbol{y}^T A \\boldsymbol{x} - \\boldsymbol{y}^T \\boldsymbol{y}) + \\mu\\boldsymbol{x}^T\\boldsymbol{x} \\\\\n",
    "&= \\frac{1}{2} \\boldsymbol{x}^T A^T A \\boldsymbol{x} - \\boldsymbol{y}^T A \\boldsymbol{x} -\n",
    "\\frac{1}{2} \\boldsymbol{y}^T \\boldsymbol{y} + \\mu\\boldsymbol{x}^T\\boldsymbol{x} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Taking the gradient of this we find\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla f(\\boldsymbol{x})\n",
    "&= \\frac{1}{2} \\nabla (\\boldsymbol{x}^T A^T A \\boldsymbol{x}) - \\boldsymbol{y}^T \\nabla( A \\boldsymbol{x}) -\n",
    "\\frac{1}{2} \\nabla(\\boldsymbol{y}^T \\boldsymbol{y}) + \\mu\\nabla(\\boldsymbol{x}^T\\boldsymbol{x})\n",
    "\\end{aligned}$$\n",
    "\n",
    "Computing $\\nabla(\\boldsymbol{x}^T\\boldsymbol{x})$ is fairly simple using Einstein notation\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial x_r}(x_i x_i) = \\delta_{ri}x_i + x_i\\delta_{ri} = x_r + x_r = 2x_r \\iff \\nabla(\\boldsymbol{x}^T\\boldsymbol{x}) = 2\\boldsymbol{x}\n",
    "\\end{aligned}$$\n",
    "\n",
    "The other reuslts we find from **a)**, hence\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla f(\\boldsymbol{x})\n",
    "&=  A^T A \\boldsymbol{x} - \\boldsymbol{y}^T A + 2\\mu\\boldsymbol{x}\n",
    "= A^T A \\boldsymbol{x} - A^T \\boldsymbol{y} + 2\\mu\\boldsymbol{x} = A^T(A\\boldsymbol{x} - \\boldsymbol{y}) + 2\\mu\\boldsymbol{x}\n",
    "\\end{aligned}$$\n",
    "\n",
    "From here, finding the hessian is pretty straight forward\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla^2 f(\\boldsymbol{x})\n",
    "= \\nabla( A^T(A\\boldsymbol{x} - \\boldsymbol{y}) + 2\\mu\\boldsymbol{x})\n",
    "= \\nabla (A^T A \\boldsymbol{x}) - \\nabla(A^T \\boldsymbol{y}) + 2\\mu \\nabla\\boldsymbol{x}\n",
    "= A^TA + 2\\mu I\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdee14",
   "metadata": {},
   "source": [
    "### PROBLEM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c423b",
   "metadata": {},
   "source": [
    "**a)**\n",
    "\n",
    "We have that $A = D + L + U$ where $D$ is the diagonal, $L$ is the lower triangular and $U$ is the upper triangular part. Also\n",
    "\n",
    "$$\\begin{aligned}\n",
    "A &= \\begin{bmatrix} C&I\\\\I&C \\end{bmatrix}, \n",
    "C = \\begin{bmatrix}\n",
    "c_1&0&\\cdots&0\\\\0&c_2&&\\vdots\\\\\\vdots&&\\ddots&\\\\0&\\cdots&&c_n \n",
    "\\end{bmatrix}\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\\begin{aligned}\n",
    "D = \\begin{bmatrix}\n",
    "c_1&0&\\cdots&&&0 \\\\ 0&\\ddots&&&&\\vdots \\\\ \\vdots&&c_n&&& \\\\ &&&c_1&& \\\\ &&&&\\ddots& \\\\ 0&\\cdots&&&&c_n\n",
    "\\end{bmatrix} \\implies\n",
    "D^{-1} = \\begin{bmatrix}\n",
    "1/c_1&0&\\cdots&&&0 \\\\ 0&\\ddots&&&&\\vdots \\\\ \\vdots&&1/c_n&&& \\\\ &&&1/c_1&& \\\\ &&&&\\ddots& \\\\ 0&\\cdots&&&&1/c_n\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "L + U = \\begin{bmatrix}\n",
    "0&0&\\cdots&1&0&0 \\\\ 0&\\ddots&&0&\\ddots&0 \\\\ \\vdots&&&0&0&1 \\\\ 1&0&0&&&\\vdots \\\\ 0&\\ddots&0&&\\ddots&0 \\\\ 0&0&1&\\cdots&0&0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally, we can calculate $B_J = D^{-1}(L+U)$ using our results.\n",
    "\n",
    "$$\n",
    "B_J =\n",
    "\\begin{bmatrix}\n",
    "1/c_1&0&\\cdots&&&0 \\\\ 0&\\ddots&&&&\\vdots \\\\ \\vdots&&1/c_n&&& \\\\ &&&1/c_1&& \\\\ &&&&\\ddots& \\\\ 0&\\cdots&&&&1/c_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0&0&\\cdots&1&0&0 \\\\ 0&\\ddots&&0&\\ddots&0 \\\\ \\vdots&&&0&0&1 \\\\ 1&0&0&&&\\vdots \\\\ 0&\\ddots&0&&\\ddots&0 \\\\ 0&0&1&\\cdots&0&0\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "0&0&\\cdots&1/c_1&0&0 \\\\ 0&\\ddots&&0&\\ddots&0 \\\\ \\vdots&&&0&0&1/c_n \\\\ 1/c_1&0&0&&&\\vdots \\\\ 0&\\ddots&0&&\\ddots&0 \\\\ 0&0&1/c_n&\\cdots&0&0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751f3a4",
   "metadata": {},
   "source": [
    "**b)**\n",
    "\n",
    "From **a)** we know that \n",
    "\n",
    "$$B_J = \\begin{bmatrix} 0&C^{-1}\\\\C^{-1}&0 \\end{bmatrix}\n",
    "\\sim D_{B_J} = \\begin{bmatrix} C^{-1}&0 \\\\ 0&C^{-1} \\end{bmatrix}$$\n",
    "\n",
    "Since $B_J$ and $D_{B_J}$ are similar, they have the same eigenvalues. $D_{B_J}$ is obviously a diagonal matrix where the diagonal is $\\{\\frac{1}{c_1}, \\frac{1}{c_2}, \\ldots, \\frac{1}{c_n}\\}$ repeated once. Since $D_{B_J}$ is diagonal, its eigenvalues are the diagonal elements themselves. Looking at the convergence criteria we get\n",
    "\n",
    "$$\n",
    "\\rho(B_J) = \\rho(D_{B_J}) = max\\{ |\\lambda_1|, \\ldots, |\\lambda_n| \\} < 1 \n",
    "\\implies |\\frac{1}{c_j}| < 1 \\hspace{0.5cm} \\forall \\hspace{0.15cm} j \\in {1,2,\\ldots,n}\n",
    "$$\n",
    "\n",
    "Finally, to get convergece we find that\n",
    "\n",
    "$$\n",
    "c_j \\in (-\\infty, -1) \\cup (1, \\infty) \\hspace{0.5cm} \\forall \\hspace{0.15cm} j \\in {1,2,\\ldots,n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d268fa5",
   "metadata": {},
   "source": [
    "**c)**\n",
    "\n",
    "We know that\n",
    "$$\\begin{aligned}\n",
    "A &= \\begin{bmatrix} C&I\\\\I&C \\end{bmatrix}, \n",
    "C = \\begin{bmatrix}\n",
    "c_1&0&\\cdots&0\\\\0&c_2&&\\vdots\\\\\\vdots&&\\ddots&\\\\0&\\cdots&&c_n \n",
    "\\end{bmatrix}\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "This means that each row of A will contain two numbers: $1$ and $c_{i}$, where $c_{i}$ is on the diagonal. If A is strictly diagonally dominant, $|c_{i}|>1$, which is the same criteria we found in b). This method of determining convergence seems easier to apply than finding the eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac113c0",
   "metadata": {},
   "source": [
    "### PROBLEM 3\n",
    "\n",
    "First we import necessary algorithms, and define the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71583879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define variables\n",
    "n = 2000        # size of A\n",
    "tol = 10**-7    # error tolerance\n",
    "maxiter = 5000  # maximum number of iterations\n",
    "b = np.ones(n)  # initiate b\n",
    "\n",
    "# Initiate and fill in A according to problem set\n",
    "A = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        A[i,j] = 3**(-np.abs(float(i-j))) + 2**(float(-i-j)) + 10**-6*np.random.uniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012e4f6",
   "metadata": {},
   "source": [
    "We implement the 3 algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49dd2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def richardson(A, b, n, tol, maxiter, omega):\n",
    "    x = b              # initial value, x = x0 = b \n",
    "    r0 = b - A@x       # initial difference r0\n",
    "    k = 0              # number of iterations\n",
    "    rel_err = tol + 1  # relative error is initially assumed larger than tol\n",
    "    \n",
    "    # Calculate new x until rel_err is small or k is larger than maxiter\n",
    "    while rel_err > tol and k < maxiter:\n",
    "        r = b - A@x\n",
    "        x = x + r*omega\n",
    "        \n",
    "        # Find new relative error, increase number of iterations by 1\n",
    "        rel_err = np.linalg.norm(r)/np.linalg.norm(r0)\n",
    "        k += 1\n",
    "    return x, k, rel_err\n",
    "\n",
    "def gaussseidel(A, b, n, tol, maxiter):\n",
    "    x = b              # initial value, x = x0 = b \n",
    "    r0 = b - A@x       # initial difference r0\n",
    "    k = 0              # number of iterations\n",
    "    rel_err = tol + 1  # relative error is initially assumed larger than tol\n",
    "    \n",
    "    # Calculate new x until rel_err is small or k is larger than maxiter\n",
    "    while rel_err > tol and k < maxiter:\n",
    "        x_new = np.zeros(n) # initiate x_{k+1}\n",
    "        for j in range(n):\n",
    "            x_new[j] = 1/A[j,j] * (b[j] - A[j,0:j]@x_new[0:j] - A[j,j+1:n]@x[j+1:n])\n",
    "        \n",
    "        # Update x to equal the new x\n",
    "        x = x_new\n",
    "        \n",
    "        # Find new relative error, increase number of iterations by 1\n",
    "        rel_err = np.linalg.norm(b-A@x)/np.linalg.norm(r0)\n",
    "        k += 1\n",
    "    return x, k, rel_err\n",
    "\n",
    "def steepestdescent(A, b, n, tol, maxiter):\n",
    "    x = b              # initial value, x = x0 = b \n",
    "    r0 = b - A@x       # initial difference r0\n",
    "    r = r0             # initiate r\n",
    "    k = 0              # number of iterations\n",
    "    rel_err = tol + 1  # relative error is initially assumed larger than tol\n",
    "\n",
    "    # Calculate new x until rel_err is small or k is larger than maxiter\n",
    "    while rel_err > tol and k < maxiter:\n",
    "        omega = (r.T@r) / (r.T@A@r)\n",
    "        x = x + omega*r\n",
    "        r = r - omega*A@r\n",
    "        \n",
    "        # Find new relative error, increase number of iterations by 1\n",
    "        rel_err = np.linalg.norm(r)/np.linalg.norm(r0)\n",
    "        k += 1\n",
    "    return x, k, rel_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a85905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method            Runtime      k        Relative error\n",
      "Richardson:       0.02329      29       9.255061291031747e-08\n",
      "Gauss-Seidel:     0.10983      15       6.973427470067081e-08\n",
      "Gradient descent: 0.09376      24       8.341099665456486e-08\n"
     ]
    }
   ],
   "source": [
    "r_start = time.time()\n",
    "x_r, k_r, err_r = richardson(A, b, n, tol, maxiter, 0.5)\n",
    "r_end = time.time()\n",
    "\n",
    "gs_start = time.time()\n",
    "x_gs, k_gs, err_gs = gaussseidel(A, b, n, tol, maxiter)\n",
    "gs_end = time.time()\n",
    "\n",
    "sd_start = time.time()\n",
    "x_sd, k_sd, err_sd = steepestdescent(A, b, n, tol, maxiter)\n",
    "sd_end = time.time()\n",
    "\n",
    "print(f\"Method            Runtime      k        Relative error\")\n",
    "print(f\"Richardson:       {(r_end-r_start):.5f}      {k_r}       {err_r}\")\n",
    "print(f\"Gauss-Seidel:     {(gs_end-gs_start):.5f}      {k_gs}       {err_gs}\")\n",
    "print(f\"Gradient descent: {(sd_end-sd_start):.5f}      {k_sd}       {err_sd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305d715",
   "metadata": {},
   "source": [
    "There is a clear correlation between runtime, number of iterations and the final relative error. The Richardson method results in the lowest runtime, but the highest amount of iterations and the largest relative error. The gradient descent method results in a longer runtime, but fewer iterations and a lower relative error. Gauss-Seidel takes the longest to run, but uses fewer iterations and has a lower relative error in the end than the two other methods, as it converges faster per step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
